{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrnavYXvj/H5oGvjdGeGqk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/akkkiii08/Deep-Learning/blob/main/Tranformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformers in Deep Learning**\n",
        "\n",
        "Transformers in deep learning represent a significant advancement in handling sequential data, particularly for tasks such as language translation and text generation. Unlike traditional recurrent neural networks (RNNs), transformers process entire sequences simultaneously through a mechanism known as \"self-attention.\" This allows them to dynamically prioritize relevant portions of input data, irrespective of their position within the sequence.\n",
        "\n",
        "This innovative approach empowers transformers to effectively capture intricate dependencies and relationships across long distances within the sequence. It has sparked a revolution in natural language processing, giving rise to formidable models like BERT and GPT, which excel in comprehending and generating human-like text.\n",
        "\n",
        "Let's delve into an example program that showcases the utilization of a transformer model for sequence classification using the PyTorch library."
      ],
      "metadata": {
        "id": "Qy1YlKbG_zi1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4TD3n_7hZog",
        "outputId": "302fb97e-d709-441f-d3f6-68456d341cf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.18.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.5.82)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "pip install torch torchvision torchtext\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "iHm-NuKLilIf"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data\n",
        "data = [\n",
        "    (\"I love this movie\", 1),\n",
        "    (\"This film was terrible\", 0),\n",
        "    (\"Great acting and plot\", 1),\n",
        "    (\"I did not like this movie\", 0),\n",
        "    (\"Fantastic experience\", 1),\n",
        "    (\"Not my type of film\", 0),\n",
        "    (\"I enjoyed every moment\", 1),\n",
        "    (\"It was a waste of time\", 0),\n",
        "    (\"Brilliant storytelling\", 1),\n",
        "    (\"Awful direction and script\", 0)\n",
        "]\n",
        "\n",
        "# Splitting data into training and testing sets\n",
        "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "class SimpleTextDataset(Dataset):\n",
        "    def __init__(self, data, vocab=None, tokenizer=lambda x: x.split()):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab or self.build_vocab(data)\n",
        "\n",
        "    def build_vocab(self, data):\n",
        "        vocab = {\"<pad>\": 0, \"<unk>\": 1}\n",
        "        idx = 2\n",
        "        for text, _ in data:\n",
        "            for token in self.tokenizer(text):\n",
        "                if token not in vocab:\n",
        "                    vocab[token] = idx\n",
        "                    idx += 1\n",
        "        return vocab\n",
        "\n",
        "    def encode(self, text):\n",
        "        return [self.vocab.get(token, self.vocab[\"<unk>\"]) for token in self.tokenizer(text)]\n",
        "\n",
        "    def pad_sequence(self, seq, max_len):\n",
        "        return seq + [0] * (max_len - len(seq))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.data[idx]\n",
        "        encoded_text = self.encode(text)\n",
        "        return torch.tensor(encoded_text), torch.tensor(label)\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = SimpleTextDataset(train_data)\n",
        "test_dataset = SimpleTextDataset(test_data, vocab=train_dataset.vocab)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    texts, labels = zip(*batch)\n",
        "    max_len = max(len(text) for text in texts)\n",
        "    padded_texts = [train_dataset.pad_sequence(text.tolist(), max_len) for text in texts]\n",
        "    return torch.tensor(padded_texts), torch.tensor(labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=collate_fn)\n"
      ],
      "metadata": {
        "id": "zYqA5RgJirRs"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, num_heads, hidden_dim, num_layers, num_classes, max_seq_length):\n",
        "        super(TransformerModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = self.generate_positional_encoding(embed_size, max_seq_length)\n",
        "        encoder_layers = nn.TransformerEncoderLayer(embed_size, num_heads, hidden_dim)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(embed_size, num_classes)\n",
        "\n",
        "    def generate_positional_encoding(self, embed_size, max_seq_length):\n",
        "        pe = torch.zeros(max_seq_length, embed_size)\n",
        "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-np.log(10000.0) / embed_size))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, src, src_mask=None):\n",
        "        src = self.embedding(src) + self.positional_encoding[:src.size(0), :]\n",
        "        output = self.transformer_encoder(src, src_mask)\n",
        "        output = self.fc(output.mean(dim=0))\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(train_dataset.vocab)\n",
        "embed_size = 64\n",
        "num_heads = 2\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = 2\n",
        "max_seq_length = 20  # Set a maximum sequence length\n",
        "\n",
        "# Instantiate the model\n",
        "model = TransformerModel(vocab_size, embed_size, num_heads, hidden_dim, num_layers, num_classes, max_seq_length)\n",
        "print(model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbtgz7nEmM-v",
        "outputId": "07b38490-1399-4ec5-bf99-987cc28697ff"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerModel(\n",
            "  (embedding): Embedding(31, 64)\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-1): 2 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=64, out_features=64, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=64, out_features=128, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=128, out_features=64, bias=True)\n",
            "        (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
      ],
      "metadata": {
        "id": "scclRqSUmTV_"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train(model, iterator, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for texts, labels in iterator:\n",
        "        optimizer.zero_grad()\n",
        "        texts = texts.permute(1, 0)  # Transformer expects (seq_len, batch_size)\n",
        "        output = model(texts)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(iterator)\n",
        "\n",
        "# Number of epochs\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, criterion, optimizer)\n",
        "    end_time = time.time()\n",
        "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {train_loss:.4f}, Time: {end_time-start_time:.2f}s')\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "id": "qyBL3AHunZTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b762323-dacc-4660-aa37-39cc177536c1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.6745, Time: 0.14s\n",
            "Epoch 2/10, Loss: 0.5714, Time: 0.03s\n",
            "Epoch 3/10, Loss: 0.4862, Time: 0.03s\n",
            "Epoch 4/10, Loss: 0.3199, Time: 0.03s\n",
            "Epoch 5/10, Loss: 0.1926, Time: 0.03s\n",
            "Epoch 6/10, Loss: 0.1658, Time: 0.03s\n",
            "Epoch 7/10, Loss: 0.1045, Time: 0.03s\n",
            "Epoch 8/10, Loss: 0.0330, Time: 0.03s\n",
            "Epoch 9/10, Loss: 0.0149, Time: 0.03s\n",
            "Epoch 10/10, Loss: 0.0130, Time: 0.03s\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    total_correct = 0\n",
        "    with torch.no_grad():\n",
        "        for texts, labels in iterator:\n",
        "            texts = texts.permute(1, 0)  # Transformer expects (seq_len, batch_size)\n",
        "            output = model(texts)\n",
        "            loss = criterion(output, labels)\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            total_correct += (predicted == labels).sum().item()\n",
        "    return total_loss / len(iterator), total_correct / len(test_dataset)\n",
        "\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "print(f'Loss on the test dataset: {test_loss:.4f}, Accuracy: {test_acc * 100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ioEtsxpgxWY",
        "outputId": "7851be04-9763-47e9-8c69-a4372b824a71"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss on the test dataset: 0.1262, Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion :\n",
        "This project demonstrates the basics of using Transformer models for text classification with PyTorch.\n",
        "In this project, following has been done:\n",
        "\n",
        "Created a simple synthetic dataset for text classification.\n",
        "Defined and trained a Transformer model using PyTorch.\n",
        "Evaluated the model's performance on the test dataset.\n"
      ],
      "metadata": {
        "id": "z-zGDAPCg-rJ"
      }
    }
  ]
}